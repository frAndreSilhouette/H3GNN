{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the raw data into city level data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "poi_data_1= pd.read_csv('yongliu_gowalla_data/gowalla_spots_subset1.csv')\n",
    "print(len(poi_data_1), 'rows of data')\n",
    "poi_data_1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import chardet\n",
    "\n",
    "# Read the first few lines of the file to detect the encoding\n",
    "with open('yongliu_gowalla_data/gowalla_spots_subset2.csv', 'rb') as f:\n",
    "    raw_data = f.read(200000)\n",
    "    result = chardet.detect(raw_data)\n",
    "\n",
    "# get the encoding type\n",
    "encoding = result['encoding']\n",
    "print(f\"Detected encoding: {encoding}\")\n",
    "poi_data_2= pd.read_csv('yongliu_gowalla_data/gowalla_spots_subset2.csv',encoding='Windows-1252')\n",
    "print(len(poi_data_2),'rows of data')\n",
    "poi_data_2\n",
    "\n",
    "# It can be seen that the number of rows of poi_data_1 and data_2 is different, indicating that their data are consistent"
   ],
   "id": "93a5da862394fe7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use map and lambda to skip NaNs and extract the first element of the non-NaN item\n",
    "city_all = list(map(lambda x: x.split(',')[0] if not pd.isna(x) else None, list(set(poi_data_2['city_state']))))\n",
    "# Remove None items from the result\n",
    "city_all = [item for item in city_all if item is not None]\n",
    "len(city_all)"
   ],
   "id": "9de1bba01c0f032e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Match Chicago\n",
    "# Select by latitude and longitude\n",
    "# To filter again by latitude and longitude, just select stores within 25km of the city center\n",
    "# ber_lat_lon_center= ( 52.5190838018783, 13.401522103237626) # Berlin Cathedral\n",
    "chi_lat_lon_center= ( 41.87950259199219, -87.6225409728181) # Art Institute of Chicago\n",
    "\n",
    "# Formula for calculating distance based on longitude and latitude\n",
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    # Radius of Earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    R = 6371.0\n",
    "\n",
    "    # Calculate the distance\n",
    "    distance = R * c\n",
    "\n",
    "    return distance  # The distance returned is in km\n",
    "\n",
    "# Calculate distance for each business_id\n",
    "poi_data_1['distance_to_center'] = poi_data_1.apply(\n",
    "    lambda row: haversine(row['lat'], row['lng'], chi_lat_lon_center[0], chi_lat_lon_center[1]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the DataFrame with the new distance column\n",
    "# Filter out rows where 'distance_to_center' is greater than 25\n",
    "chi_filtered_poi_1 = poi_data_1[poi_data_1['distance_to_center'] <= 25]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "chi_filtered_poi_1 = chi_filtered_poi_1.reset_index(drop=True)\n",
    "len(chi_filtered_poi_1)"
   ],
   "id": "9f80b9fd4a33f64e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Delete the rows where the count column is less than 10. For the Berlin data, you donâ€™t need to delete it, and the total amount is not large.\n",
    "chi_filtered_poi_1 = chi_filtered_poi_1[chi_filtered_poi_1['checkins_count'] >= 10]\n",
    "chi_filtered_poi_1_list = list(chi_filtered_poi_1['id'])\n",
    "len(chi_filtered_poi_1_list)"
   ],
   "id": "e43d014ff0dbbc5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "poi_list = list(chi_filtered_poi_1['id'])\n",
    "# Now we can find users based on the checkins on these POIs.\n",
    "\n",
    "check_in_data =pd.read_csv('yongliu_gowalla_data/gowalla_checkins.csv')\n",
    "check_in_data"
   ],
   "id": "e112bf894080b51d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "check_in_chi = check_in_data[check_in_data['placeid'].isin(poi_list)]\n",
    "print(len(check_in_chi))\n",
    "check_in_chi"
   ],
   "id": "5d1c8f646b749bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_list = list(set(check_in_chi['userid']))\n",
    "len(user_list)"
   ],
   "id": "502bb34918962e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "friend_ship = pd.read_csv('yongliu_gowalla_data/gowalla_friendship.csv')\n",
    "friend_ship"
   ],
   "id": "d1e7804f51a98f37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Filter friend_ship and keep rows that meet the condition\n",
    "filtered_friend_ship = friend_ship[\n",
    "    (friend_ship['userid1'].isin(user_list)) & (friend_ship['userid2'].isin(user_list))\n",
    "]\n",
    "filtered_friend_ship = filtered_friend_ship.reset_index(drop=True)\n",
    "print(len(filtered_friend_ship),'friendship edges')"
   ],
   "id": "4d67e05dd743f6b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(filtered_friend_ship)",
   "id": "4670691e888f8c5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filtered_friend_ship",
   "id": "a32f040578527837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a new column 'user_pair', which is a sorted combination of (userid1, userid2)\n",
    "filtered_friend_ship['user_pair'] = filtered_friend_ship.apply(lambda row: tuple(sorted([row['userid1'], row['userid2']])), axis=1)\n",
    "\n",
    "# Check if there are duplicate 'user_pair' entries\n",
    "duplicates = filtered_friend_ship[filtered_friend_ship.duplicated('user_pair', keep=False)]\n",
    "\n",
    "# If duplicates exist, output the duplicate records\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate user pairs exist:\", len(duplicates))  # Indicates that this friendship is stored bilaterally\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicate user pairs.\")\n",
    "\n",
    "print(len(duplicates)/2, \" unique pairs, as a hypergraph does not require duplicate pairs\")\n"
   ],
   "id": "aa4e634a2c2c3fb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Only keep the first occurrence of a duplicate, and delete subsequent duplicates\n",
    "friend_ship_unique = filtered_friend_ship.drop_duplicates('user_pair', keep='first')\n",
    "len(friend_ship_unique)"
   ],
   "id": "30b3198c77d9f8c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import itertools\n",
    "user_list_in_friend_ship = list( set( list(itertools.chain(list( friend_ship_unique['userid1']  ), list( friend_ship_unique['userid2']  )))))\n",
    "print(len(user_list_in_friend_ship),'appeared in the friend network')"
   ],
   "id": "2cb4591a81afe5ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Filter check_in again based on this list\n",
    "check_in_chi_user_in_friend = check_in_chi[check_in_chi['userid'].isin(user_list_in_friend_ship)]\n",
    "print(len(check_in_chi_user_in_friend),'check-in records, filtered')"
   ],
   "id": "7deb709c7fa33b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "check_in_chi_user_in_friend",
   "id": "40cca7d092a91937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "check_in_chi_user_in_friend.to_csv('yongliu_gowalla_data/Chicago/check_in_chi_user_in_friend.csv', index=False)\n",
    "friend_ship_unique.to_csv('yongliu_gowalla_data/Chicago/friend_ship_chi.csv', index=False)"
   ],
   "id": "fcc53fa7221a0fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Further filter the POI information for Chicago\n",
    "\n",
    "chi_filtered_poi_1 = chi_filtered_poi_1[chi_filtered_poi_1['id'].isin(set(check_in_chi_user_in_friend['placeid']))]\n",
    "print(len(chi_filtered_poi_1), \"POIs appeared in the check-in records, and the users of these check-ins have a friendship network\")\n",
    "chi_filtered_poi_1.to_csv('yongliu_gowalla_data/Chicago/chi_poi_incheckin_and_friend.csv', index=False)"
   ],
   "id": "65e5f6e2cbb83c40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
